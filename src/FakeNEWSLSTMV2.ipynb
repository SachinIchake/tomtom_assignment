{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FakeNEWSLSTMV2.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1WbsmNnYKsAJsTuhCayORGSb8l801xlm6","authorship_tag":"ABX9TyNsUTTmeAZHRG+R+xFZk8Sl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"dh7MbCTBmwwo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595179763033,"user_tz":-330,"elapsed":2039,"user":{"displayName":"Sachin Ichake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjga3WUVn0RezjXis_F_4AkPnCWPcJ94q37BrAFDA=s64","userId":"08081466110737310745"}}},"source":["import numpy as np\n","import pandas as pd\n","import os\n","import time\n","import gc\n","import random\n","from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n","from keras.preprocessing import text, sequence\n","import torch\n","from torch import nn\n","from torch.utils import data\n","from sklearn import metrics\n","from torch.nn import functional as F"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"id":"FmryB5dtm1nv","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595179764744,"user_tz":-330,"elapsed":2665,"user":{"displayName":"Sachin Ichake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjga3WUVn0RezjXis_F_4AkPnCWPcJ94q37BrAFDA=s64","userId":"08081466110737310745"}}},"source":["\n","# disable progress bars when submitting\n","def is_interactive():\n","   return 'SHLVL' not in os.environ\n","\n","if not is_interactive():\n","    def nop(it, *a, **k):\n","        return it\n","\n","    tqdm = nop\n","\n","\n","\n","# def seed_everything(seed=1234):\n","#     random.seed(seed)\n","#     os.environ['PYTHONHASHSEED'] = str(seed)\n","#     np.random.seed(seed)\n","#     torch.manual_seed(seed)\n","#     torch.cuda.manual_seed(seed)\n","#     torch.backends.cudnn.deterministic = True\n","# seed_everything()\n"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"id":"o1x-nCMgnZ4y","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595180476660,"user_tz":-330,"elapsed":2546,"user":{"displayName":"Sachin Ichake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjga3WUVn0RezjXis_F_4AkPnCWPcJ94q37BrAFDA=s64","userId":"08081466110737310745"}}},"source":["MAX_LEN = 256\n","TRAIN_BATCH_SIZE = 256\n","TEST_BATCH_SIZE = 128\n","EPOCHS = 1\n","MODEL_PATH = \"/content/drive/My Drive/datasets/data/modelv2.pt\"\n","TRAINING_FILE = \"/content/drive/My Drive/datasets/data/train.csv\"\n","NEW_TRAINING_FILE = \"/content/drive/My Drive/datasets/data/train.csv\"\n","TESTING_FILE = \"/content/drive/My Drive/datasets/data/test.csv\"\n","GLOVE = \"/content/drive/My Drive/datasets/data/glove.6B.300d.txt\"\n","SUBMIT_FILE ='/content/drive/My Drive/datasets/data/submit.csv'"],"execution_count":76,"outputs":[]},{"cell_type":"code","metadata":{"id":"BzHBVBUEm3w3","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WTTYLTnamzWM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595179776529,"user_tz":-330,"elapsed":4630,"user":{"displayName":"Sachin Ichake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjga3WUVn0RezjXis_F_4AkPnCWPcJ94q37BrAFDA=s64","userId":"08081466110737310745"}}},"source":["\n","\n","# CRAWL_EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n","GLOVE_EMBEDDING_PATH = GLOVE\n","NUM_MODELS = 1\n","LSTM_UNITS = 256\n","DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n","MAX_LEN = 512\n","\n"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"JtO5mRVfm7Wm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595181854990,"user_tz":-330,"elapsed":3265,"user":{"displayName":"Sachin Ichake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjga3WUVn0RezjXis_F_4AkPnCWPcJ94q37BrAFDA=s64","userId":"08081466110737310745"}}},"source":["\n","def get_coefs(word, *arr):\n","    return word, np.asarray(arr, dtype='float32')\n","\n","def load_embeddings(path):\n","    with open(path,encoding='utf-8') as f:\n","        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n","\n","def build_matrix(word_index, path):\n","    embedding_index = load_embeddings(path)\n","    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n","    unknown_words = []\n","    \n","    for word, i in word_index.items():\n","        try:\n","            embedding_matrix[i] = embedding_index[word]\n","        except KeyError:\n","            unknown_words.append(word)\n","    return embedding_matrix, unknown_words\n","\n","\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n","                batch_size=512, n_epochs=EPOCHS,\n","                enable_checkpoint_ensemble=True):\n","    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n","    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n","\n","    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n","    \n","    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n","    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n","    all_test_preds = []\n","    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n","    best_accuracy = 0\n","    for epoch in range(n_epochs):\n","        start_time = time.time()\n","        \n","        scheduler.step()\n","        \n","        model.train()\n","        avg_loss = 0.\n","        \n","        for data in tqdm(train_loader, disable=False):\n","            x_batch = data[:-1]\n","            y_batch = data[-1]\n","\n","            y_pred = model(*x_batch)            \n","            loss = loss_fn(y_pred, y_batch)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","\n","            optimizer.step()\n","            avg_loss += loss.item() / len(train_loader)\n","            \n","        model.eval()\n","        test_preds = np.zeros((len(test), output_dim))\n","    \n","        for i, x_batch in enumerate(test_loader):\n","            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy())\n","\n","            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n","\n","        all_test_preds.append(test_preds)\n","        elapsed_time = time.time() - start_time\n","        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n","              epoch + 1, n_epochs, avg_loss, elapsed_time))\n","\n","        if enable_checkpoint_ensemble:\n","          test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n","        else:\n","          test_preds = all_test_preds[-1]\n","\n","        \n","        prediction=np.mean(all_test_preds, axis=0)[:, 0]\n","        outputs = (np.array(prediction) >= 0.5)*1\n","        # submission = pd.DataFrame.from_dict({\n","        #     'id': test['id'],\n","        #     'prediction': prediction,\n","        #     'outputs':outputs\n","        # })\n","\n","        accuracy = metrics.accuracy_score(df_submit['label'],prediction)\n","        print(f\"Accuracy Score = {accuracy}\")\n","\n","        # if accuracy > best_accuracy:\n","        #     torch.save(model.state_dict(), MODEL_PATH)\n","        #     best_accuracy = accuracy\n","        #     submission.to_csv('/content/drive/My Drive/datasets/data/submission.csv', index=False)\n","\n","\n","    # if enable_checkpoint_ensemble:\n","    #     test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n","    # else:\n","    #     test_preds = all_test_preds[-1]\n","        \n","    return test_preds\n"],"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"id":"f2u48hWtm_De","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595181857493,"user_tz":-330,"elapsed":2577,"user":{"displayName":"Sachin Ichake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjga3WUVn0RezjXis_F_4AkPnCWPcJ94q37BrAFDA=s64","userId":"08081466110737310745"}}},"source":["\n","\n","class SpatialDropout(nn.Dropout2d):\n","    def forward(self, x):\n","        x = x.unsqueeze(2)    # (N, T, 1, K)\n","        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n","        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n","        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n","        x = x.squeeze(2)  # (N, T, K)\n","        return x\n","    \n","class NeuralNet(nn.Module):\n","    def __init__(self, embedding_matrix, num_aux_targets):\n","        super(NeuralNet, self).__init__()\n","        embed_size = embedding_matrix.shape[1]\n","        \n","        self.embedding = nn.Embedding(max_features, embed_size)\n","        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = False\n","        self.embedding_dropout = SpatialDropout(0.3)\n","        \n","        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n","        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n","    \n","        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n","        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n","        \n","        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n","        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n","        \n","    def forward(self, x):\n","        h_embedding = self.embedding(x)\n","        h_embedding = self.embedding_dropout(h_embedding)\n","        \n","        h_lstm1, _ = self.lstm1(h_embedding)\n","        h_lstm2, _ = self.lstm2(h_lstm1)\n","        \n","        # global average pooling\n","        avg_pool = torch.mean(h_lstm2, 1)\n","        # global max pooling\n","        max_pool, _ = torch.max(h_lstm2, 1)\n","        \n","        h_conc = torch.cat((max_pool, avg_pool), 1)\n","        h_conc_linear1  = F.relu(self.linear1(h_conc))\n","        h_conc_linear2  = F.relu(self.linear2(h_conc))\n","        \n","        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n","        \n","        result = self.linear_out(hidden)\n","        aux_result = self.linear_aux_out(hidden)\n","        out = torch.cat([result, aux_result], 1)\n","        \n","        return out\n"],"execution_count":92,"outputs":[]},{"cell_type":"code","metadata":{"id":"orlDWjJCnBuO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595181862846,"user_tz":-330,"elapsed":1741,"user":{"displayName":"Sachin Ichake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjga3WUVn0RezjXis_F_4AkPnCWPcJ94q37BrAFDA=s64","userId":"08081466110737310745"}}},"source":["\n","    \n","def preprocess(data):\n","    '''\n","    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n","    '''\n","    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n","    def clean_special_chars(text, punct):\n","        for p in punct:\n","            text = text.replace(p, ' ')\n","        return text\n","\n","    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n","    return data\n"],"execution_count":93,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTkt6gDtnEVA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":517},"executionInfo":{"status":"error","timestamp":1595181964744,"user_tz":-330,"elapsed":103627,"user":{"displayName":"Sachin Ichake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjga3WUVn0RezjXis_F_4AkPnCWPcJ94q37BrAFDA=s64","userId":"08081466110737310745"}},"outputId":"8ec79e9d-7507-4179-bc18-946fbed4cf43"},"source":["\n","\n","train = pd.read_csv(TRAINING_FILE)\n","test = pd.read_csv(TESTING_FILE)\n","df_submit= pd.read_csv(SUBMIT_FILE)\n","\n","\n","x_train = preprocess(train['text'])\n","y_train = train['label'] \n","y_aux_train = train[['label']]\n","x_test = preprocess(test['text'])\n","\n","max_features = None\n","\n","\n","\n","tokenizer = text.Tokenizer()\n","tokenizer.fit_on_texts(list(x_train) + list(x_test))\n","\n","x_train = tokenizer.texts_to_sequences(x_train)\n","x_test = tokenizer.texts_to_sequences(x_test)\n","x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n","x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n","\n","\n","max_features = max_features or len(tokenizer.word_index) + 1\n","max_features\n","\n","\n","\n","\n","# crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n","# print('n unknown words (crawl): ', len(unknown_words_crawl))\n","\n","\n","\n","\n","\n","glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n","print('n unknown words (glove): ', len(unknown_words_glove))\n","\n","\n","\n","\n","\n","embedding_matrix = glove_matrix\n","embedding_matrix.shape\n","\n","# del crawl_matrix\n","del glove_matrix\n","gc.collect()\n","\n","\n","\n","\n","x_train_torch = torch.tensor(x_train, dtype=torch.long).cuda()\n","x_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\n","y_train_torch = torch.tensor(np.hstack([y_train[:, np.newaxis], y_aux_train]), dtype=torch.float32).cuda()\n","\n","\n","\n","train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\n","test_dataset = data.TensorDataset(x_test_torch)\n","\n","all_test_preds = []\n","\n","for model_idx in range(NUM_MODELS):\n","    print('Model ', model_idx)\n","    # seed_everything(1234 + model_idx)\n","    \n","    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n","    model.cuda()\n","    \n","    test_preds = train_model(model, train_dataset, test_dataset, output_dim=y_train_torch.shape[-1], \n","                             loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n","    all_test_preds.append(test_preds)\n","    print()\n","\n","\n","\n"],"execution_count":94,"outputs":[{"output_type":"stream","text":["n unknown words (glove):  99907\n","Model  0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/1 \t loss=0.4883 \t time=34.23s\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-94-859a18ea4a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     test_preds = train_model(model, train_dataset, test_dataset, output_dim=y_train_torch.shape[-1], \n\u001b[0;32m---> 75\u001b[0;31m                              loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mall_test_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-91-c67166d1b083>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train, test, loss_fn, output_dim, lr, batch_size, n_epochs, enable_checkpoint_ensemble)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# })\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_submit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy Score = {accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"]}]},{"cell_type":"code","metadata":{"id":"mBrt1lHyJ-Ye","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1595180774733,"user_tz":-330,"elapsed":3738,"user":{"displayName":"Sachin Ichake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjga3WUVn0RezjXis_F_4AkPnCWPcJ94q37BrAFDA=s64","userId":"08081466110737310745"}},"outputId":"026aede2-ecef-4447-e92a-ff45d22e165c"},"source":["# best_accuracy=0\n","# outputs=np.mean(all_test_preds, axis=0)[:, 0]\n","# outputs = (np.array(outputs) >= 0.5)*1\n","# submission = pd.DataFrame.from_dict({\n","#     'id': test['id'],\n","#     'prediction': np.mean(all_test_preds, axis=0)[:, 0],\n","#     'outputs':outputs\n","# })\n","\n","# accuracy = metrics.accuracy_score(df_submit.label.values,submission.outputs.values)\n","# print(f\"Accuracy Score = {accuracy}\")\n","\n","# if accuracy > best_accuracy:\n","#     torch.save(model.state_dict(), MODEL_PATH)\n","#     best_accuracy = accuracy\n","#     submission.to_csv('/content/drive/My Drive/datasets/data/submission.csv', index=False)\n","  "],"execution_count":80,"outputs":[{"output_type":"stream","text":["Accuracy Score = 0.6257692307692307\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"losLlormJ_hY","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uu71HeS44If2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595178551790,"user_tz":-330,"elapsed":3889,"user":{"displayName":"Sachin Ichake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjga3WUVn0RezjXis_F_4AkPnCWPcJ94q37BrAFDA=s64","userId":"08081466110737310745"}}},"source":["outputs=np.mean(all_test_preds, axis=0)[:, 0]\n","outputs = (np.array(outputs) >= 0.5)*1"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDcVrjwzwt5c","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595178557285,"user_tz":-330,"elapsed":1905,"user":{"displayName":"Sachin Ichake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjga3WUVn0RezjXis_F_4AkPnCWPcJ94q37BrAFDA=s64","userId":"08081466110737310745"}}},"source":["submission = pd.DataFrame.from_dict({\n","    'id': test['id'],\n","    'prediction': np.mean(all_test_preds, axis=0)[:, 0],\n","    'outputs':outputs\n","})\n","\n","submission.to_csv('/content/drive/My Drive/datasets/data/submission.csv', index=False)"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ok7b1qlmA7LP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1595178924060,"user_tz":-330,"elapsed":2434,"user":{"displayName":"Sachin Ichake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjga3WUVn0RezjXis_F_4AkPnCWPcJ94q37BrAFDA=s64","userId":"08081466110737310745"}},"outputId":"e9b3f815-a316-40b0-bce3-5d17f71c10e3"},"source":["from sklearn import metrics\n","df_submit= pd.read_csv('/content/drive/My Drive/datasets/data/submit.csv')\n","accuracy = metrics.accuracy_score(df_submit.label.values,submission.outputs.values)\n","print(f\"Accuracy Score = {accuracy}\")\n"],"execution_count":56,"outputs":[{"output_type":"stream","text":["Accuracy Score = 0.635\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3-lhT8ViGM0S","colab_type":"code","colab":{}},"source":["outputs=np.mean(all_test_preds, axis=0)[:, 0]\n","outputs = (np.array(outputs) >= 0.5)*1\n","submission = pd.DataFrame.from_dict({\n","    'id': test['id'],\n","    'prediction': np.mean(all_test_preds, axis=0)[:, 0],\n","    'outputs':outputs\n","})\n","\n","submission.to_csv('/content/drive/My Drive/datasets/data/submission.csv', index=False)\n","from sklearn import metrics\n","accuracy = metrics.accuracy_score(df_submit.label.values,submission.outputs.values)\n","print(f\"Accuracy Score = {accuracy}\")\n"],"execution_count":null,"outputs":[]}]}